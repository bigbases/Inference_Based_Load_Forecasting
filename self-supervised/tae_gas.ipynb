{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyPbGYulNv11v2nPdNWp/Vko"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJRTeFChxqT6","executionInfo":{"status":"ok","timestamp":1721963924580,"user_tz":-540,"elapsed":7536,"user":{"displayName":"최지혁","userId":"01028477956824263689"}},"outputId":"b1768968-9446-47e1-ad22-efcd99a15eb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"sm_86\"\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Capstone')\n","\n","# library\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.cluster import KMeans\n","import sklearn\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import trange, tqdm\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","from model_transformer5 import TSTransformerEncoder\n","from torch.optim.optimizer import Optimizer\n","from collections import OrderedDict\n","import math\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","from tae import *"]},{"cell_type":"code","source":["# dataset load\n","# 전기\n","elec_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/data/elec_clustering_train.csv', index_col=0)\n","elec_train.index = pd.to_datetime(elec_train.index)\n","elec_train = elec_train.resample(rule='12H').sum()\n","elec_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/data/elec_clustering_test.csv', index_col=0)\n","elec_test.index = pd.to_datetime(elec_test.index)\n","elec_test = elec_test.resample(rule='12H').sum()\n","elec_total = pd.concat([elec_train,elec_test],axis=1)\n","# 수도\n","water_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/data/water_clustering_train.csv', index_col=0)\n","water_train.index = pd.to_datetime(water_train.index)\n","water_train = water_train.resample(rule='12H').sum()\n","water_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/data/water_clustering_test.csv', index_col=0)\n","water_test.index = pd.to_datetime(water_test.index)\n","water_test = water_test.resample(rule='12H').sum()\n","water_total = pd.concat([water_train,water_test],axis=1)\n","#가스\n","gas_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/data/gas_clustering_train.csv', index_col=0)\n","gas_train.index = pd.to_datetime(gas_train.index)\n","gas_train = gas_train.resample(rule='12H').sum()\n","gas_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/data/gas_clustering_test.csv', index_col=0)\n","gas_test.index = pd.to_datetime(gas_test.index)\n","gas_test = gas_test.resample(rule='12H').sum()\n","gas_total = pd.concat([gas_train,gas_test],axis=1)\n","#온수\n","hotwater_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/data/hotwater_clustering_train.csv', index_col=0)\n","hotwater_train.index = pd.to_datetime(hotwater_train.index)\n","hotwater_train = hotwater_train.resample(rule='12H').sum()\n","hotwater_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/data/hotwater_clustering_test.csv', index_col=0)\n","hotwater_test.index = pd.to_datetime(hotwater_test.index)\n","hotwater_test = hotwater_test.resample(rule='12H').sum()\n","hotwater_total = pd.concat([hotwater_train,hotwater_test],axis=1)\n","\n","region_A = elec_train.columns\n","region_B = elec_test.columns\n","\n","train_index = 504*2\n","test_index = (504+28)*2\n","cluster_index = (504+28+28)*2\n","\n","energy_A = elec_total\n","energy_B = water_total\n","energy_C = hotwater_total\n","energy_target = gas_total\n","\n","train_A = energy_A[:train_index]\n","train_B = energy_B[:train_index]\n","train_C = energy_C[:train_index]\n","test_A = energy_A[train_index:test_index]\n","test_B = energy_B[train_index:test_index]\n","test_C = energy_C[train_index:test_index]\n","test_target = energy_target[train_index:test_index]\n","cluster_A = energy_A[test_index:cluster_index]\n","cluster_B = energy_B[test_index:cluster_index]\n","cluster_C = energy_C[test_index:cluster_index]\n","cluster_target = energy_target[test_index:cluster_index]\n","\n","scaler_A = MinMaxScaler()\n","scaler_B = MinMaxScaler()\n","scaler_C = MinMaxScaler()\n","train_A = pd.DataFrame(scaler_A.fit_transform(train_A))\n","train_B = pd.DataFrame(scaler_B.fit_transform(train_B))\n","train_C = pd.DataFrame(scaler_C.fit_transform(train_C))\n","test_A = pd.DataFrame(scaler_A.transform(test_A))\n","test_B = pd.DataFrame(scaler_B.transform(test_B))\n","test_C = pd.DataFrame(scaler_C.transform(test_C))\n","cluster_A = pd.DataFrame(scaler_A.transform(cluster_A))\n","cluster_B = pd.DataFrame(scaler_B.transform(cluster_B))\n","cluster_C = pd.DataFrame(scaler_C.transform(cluster_C))\n","\n","# 2-5\n","repr1_A = energy_A[train_index+14:test_index+14]\n","repr1_B = energy_B[train_index+14:test_index+14]\n","repr1_C = energy_C[train_index+14:test_index+14]\n","repr1_A = pd.DataFrame(scaler_A.transform(repr1_A))\n","repr1_B = pd.DataFrame(scaler_B.transform(repr1_B))\n","repr1_C = pd.DataFrame(scaler_C.transform(repr1_C))\n","\n","# 3-6\n","repr2_A = energy_A[train_index+28:test_index+28]\n","repr2_B = energy_B[train_index+28:test_index+28]\n","repr2_C = energy_C[train_index+28:test_index+28]\n","repr2_A = pd.DataFrame(scaler_A.transform(repr2_A))\n","repr2_B = pd.DataFrame(scaler_B.transform(repr2_B))\n","repr2_C = pd.DataFrame(scaler_C.transform(repr2_C))\n","\n","# 4-7\n","repr3_A = energy_A[train_index+42:test_index+42]\n","repr3_B = energy_B[train_index+42:test_index+42]\n","repr3_C = energy_C[train_index+42:test_index+42]\n","repr3_A = pd.DataFrame(scaler_A.transform(repr3_A))\n","repr3_B = pd.DataFrame(scaler_B.transform(repr3_B))\n","repr3_C = pd.DataFrame(scaler_C.transform(repr3_C))\n","\n","data_train = []\n","for user in range(elec_total.shape[1]):\n","    for index in range(0,elec_total.shape[0]//(28*2)-2):\n","        window = []\n","        for period in range(index*28*2,(index+1)*28*2):\n","            window.append([train_A.iloc[period,user],train_B.iloc[period,user],train_C.iloc[period,user]])\n","        data_train.append(window)\n","\n","data_test = []\n","for user in range(elec_total.shape[1]):\n","    for index in range(0,1):\n","        window = []\n","        for period in range(index*28*2,(index+1)*28*2):\n","            window.append([test_A.iloc[period,user],test_B.iloc[period,user],test_C.iloc[period,user]])\n","        data_test.append(window)\n","\n","data_repr1 = []\n","for user in range(elec_total.shape[1]):\n","    for index in range(0,1):\n","        window = []\n","        for period in range(index*28*2,(index+1)*28*2):\n","            window.append([repr1_A.iloc[period,user],repr1_B.iloc[period,user],repr1_C.iloc[period,user]])\n","        data_repr1.append(window)\n","\n","data_repr2 = []\n","for user in range(elec_total.shape[1]):\n","    for index in range(0,1):\n","        window = []\n","        for period in range(index*28*2,(index+1)*28*2):\n","            window.append([repr2_A.iloc[period,user],repr2_B.iloc[period,user],repr2_C.iloc[period,user]])\n","        data_repr2.append(window)\n","\n","data_repr3 = []\n","for user in range(elec_total.shape[1]):\n","    for index in range(0,1):\n","        window = []\n","        for period in range(index*28*2,(index+1)*28*2):\n","            window.append([repr3_A.iloc[period,user],repr3_B.iloc[period,user],repr3_C.iloc[period,user]])\n","        data_repr3.append(window)\n","\n","# dataset\n","data_train = np.array(data_train)\n","data_test = np.array(data_test)\n","data_repr1 = np.array(data_repr1)\n","data_repr2 = np.array(data_repr2)\n","data_repr3 = np.array(data_repr3)\n","\n","# masking\n","data_train_mask = []\n","for i in range(data_train.shape[0]):\n","    data_train_mask.append(noise_mask(data_train[i]))\n","\n","data_test_mask = []\n","for i in range(data_test.shape[0]):\n","    data_test_mask.append(noise_mask(data_test[i]))\n","\n","data_repr1_mask = []\n","for i in range(data_repr1.shape[0]):\n","    data_repr1_mask.append(noise_mask(data_repr1[i]))\n","\n","data_repr2_mask = []\n","for i in range(data_repr2.shape[0]):\n","    data_repr2_mask.append(noise_mask(data_repr2[i]))\n","\n","data_repr3_mask = []\n","for i in range(data_repr3.shape[0]):\n","    data_repr3_mask.append(noise_mask(data_repr3[i]))\n","\n","data_train_mask = np.array(data_train_mask)\n","data_test_mask = np.array(data_test_mask)\n","data_repr1_mask = np.array(data_repr1_mask)\n","data_repr2_mask = np.array(data_repr2_mask)\n","data_repr3_mask = np.array(data_repr3_mask)\n","\n","# window dataset\n","class TimeSeriesDataset(Dataset):\n","    def __init__(self, X, mask):\n","        self.X = X\n","        self.target = X\n","        self.mask = mask\n","        # self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        return self.X[index], self.target[index], self.mask[index]\n","\n","ds_train = TimeSeriesDataset(data_train, data_train_mask)\n","ds_test = TimeSeriesDataset(data_test, data_test_mask)\n","ds_repr1 = TimeSeriesDataset(data_repr1, data_repr1_mask)\n","ds_repr2 = TimeSeriesDataset(data_repr2, data_repr2_mask)\n","ds_repr3 = TimeSeriesDataset(data_repr3, data_repr3_mask)\n","\n","batch_size = 128\n","train_loader = DataLoader(dataset=ds_train, batch_size=batch_size, drop_last=True, shuffle=True)\n","test_loader = DataLoader(dataset=ds_test, batch_size=batch_size, drop_last=False, shuffle=False)\n","repr1_loader = DataLoader(dataset=ds_repr1, batch_size=batch_size, drop_last=False, shuffle=False)\n","repr2_loader = DataLoader(dataset=ds_repr2, batch_size=batch_size, drop_last=False, shuffle=False)\n","repr3_loader = DataLoader(dataset=ds_repr3, batch_size=batch_size, drop_last=False, shuffle=False)"],"metadata":{"id":"Amt_r-kUx4aY","executionInfo":{"status":"ok","timestamp":1721964011676,"user_tz":-540,"elapsed":87111,"user":{"displayName":"최지혁","userId":"01028477956824263689"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["num = 0\n","device = torch.device('cuda')\n","model = TSTransformerEncoder(feat_dim=data_train.shape[2], max_len=data_train.shape[1], d_model=64, n_heads=8, num_layers=1, dim_feedforward=256, pos_encoding='learnable')\n","model.to(device)\n","# optimizer = RAdam(model.parameters())\n","optimizer = torch.optim.AdamW(model.parameters())\n","\n","# training loop\n","model.train()\n","num_epochs = 200\n","loss_module = MaskedMSELoss()\n","loss_list = []\n","val_list = []\n","valmask_list = []\n","val_output = []\n","val_true = []\n","criterion = nn.MSELoss()\n","for epoch in tqdm(range(num_epochs)):\n","    epoch_loss = 0  # total loss of epoch\n","    total_active_elements = 0  # total unmasked elements in epoch\n","    for i, batch in enumerate(train_loader):\n","\n","        X, targets, target_masks = batch # X는 mask되지 않은 값\n","        target_masks = target_masks.to(device) # 1s: mask and predict, 0s: unaffected input (ignore) // noise_mask 에서 옴\n","        X = X.to(device)\n","        targets = torch.tensor(targets.to(device), dtype = torch.float32) # mask 되지 않은 값\n","        X = X * target_masks # mask 된 input\n","        X = torch.tensor(X, dtype = torch.float32)\n","        predictions, _ = model(X)  # (batch_size, padded_length, feat_dim)\n","\n","        # Cascade noise masks (batch_size, padded_length, feat_dim) and padding masks (batch_size, padded_length)\n","        loss = loss_module(predictions, targets, ~target_masks)  # ~target_mask를 prediction, target에 역으로 곱해서 mask한 값만 loss 계산함\n","        total_loss = loss\n","        # Zero gradients, perform a backward pass, and update the weights.\n","        optimizer.zero_grad()\n","        total_loss.backward()\n","        torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n","        optimizer.step()\n","        total_active_elements += 1\n","        epoch_loss += total_loss.item()\n","    with torch.no_grad():\n","        val_loss = 0.0\n","        valmask_loss = 0.0\n","        for i, batch in enumerate(test_loader):\n","            X, targets, target_masks = batch # X는 mask되지 않은 값\n","            target_masks = target_masks.to(device) # 1s: mask and predict, 0s: unaffected input (ignore) // noise_mask 에서 옴\n","            X = X.to(device)\n","            targets = torch.tensor(targets.to(device), dtype = torch.float32) # mask 되지 않은 값\n","            X = X * target_masks # mask 된 input\n","            X = torch.tensor(X, dtype = torch.float32)\n","            predictions, _ = model(X)\n","            val_loss += criterion(predictions, targets.float()).item()\n","            valmask_loss += loss_module(predictions, targets, ~target_masks)\n","        val_list.append(val_loss)\n","        valmask_list.append(valmask_loss)\n","        if epoch == num_epochs-1:\n","            val_true.append(targets.cpu())\n","            val_output.append(predictions.cpu())\n","    epoch_loss = epoch_loss / total_active_elements\n","    loss_list.append(epoch_loss)\n","print(epoch_loss)\n","\n","\n","repr1_hidden = []\n","with torch.no_grad():\n","    for i, batch in enumerate(repr1_loader):\n","        X, _, _ = batch # X는 mask되지 않은 값\n","        X = X.to(device)\n","        X = torch.tensor(X, dtype = torch.float32)\n","        _, hidden = model(X)\n","        repr1_hidden.append(hidden.cpu().numpy())\n","arr_hidden_1 = []\n","for i in range(9):\n","    for j in range(repr1_hidden[i].shape[0]):\n","        arr_hidden_1.append(repr1_hidden[i][j])\n","arr_hidden_1 = pd.DataFrame(np.reshape(arr_hidden_1,(1080,64*28*2)))\n","\n","repr2_hidden = []\n","with torch.no_grad():\n","    for i, batch in enumerate(repr2_loader):\n","        X, _, _ = batch # X는 mask되지 않은 값\n","        X = X.to(device)\n","        X = torch.tensor(X, dtype = torch.float32)\n","        _, hidden = model(X)\n","        repr2_hidden.append(hidden.cpu().numpy())\n","arr_hidden_2 = []\n","for i in range(9):\n","    for j in range(repr2_hidden[i].shape[0]):\n","        arr_hidden_2.append(repr2_hidden[i][j])\n","arr_hidden_2 = pd.DataFrame(np.reshape(arr_hidden_2,(1080,64*28*2)))\n","\n","repr3_hidden = []\n","with torch.no_grad():\n","    for i, batch in enumerate(repr3_loader):\n","        X, _, _ = batch # X는 mask되지 않은 값\n","        X = X.to(device)\n","        X = torch.tensor(X, dtype = torch.float32)\n","        _, hidden = model(X)\n","        repr3_hidden.append(hidden.cpu().numpy())\n","arr_hidden_3 = []\n","for i in range(9):\n","    for j in range(repr3_hidden[i].shape[0]):\n","        arr_hidden_3.append(repr3_hidden[i][j])\n","arr_hidden_3 = pd.DataFrame(np.reshape(arr_hidden_3,(1080,64*28*2)))\n","\n","\n","arr_hidden_1.index = elec_total.columns\n","arr_hidden_1.to_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/result/tae_gas/arr_hidden_{}_{}.csv'.format(1, num))\n","arr_hidden_2.index = elec_total.columns\n","arr_hidden_2.to_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/result/tae_gas/arr_hidden_{}_{}.csv'.format(2, num))\n","arr_hidden_3.index = elec_total.columns\n","arr_hidden_3.to_csv('/content/drive/MyDrive/Colab Notebooks/Capstone/result/tae_gas/arr_hidden_{}_{}.csv'.format(3, num))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xy4sSXAfx5gV","executionInfo":{"status":"ok","timestamp":1721964238628,"user_tz":-540,"elapsed":226975,"user":{"displayName":"최지혁","userId":"01028477956824263689"}},"outputId":"356fb418-9c0d-495c-8585-0f5a04b2670d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","  0%|          | 0/200 [00:00<?, ?it/s]<ipython-input-3-cdf7f5c9cc94>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  targets = torch.tensor(targets.to(device), dtype = torch.float32) # mask 되지 않은 값\n","<ipython-input-3-cdf7f5c9cc94>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X = torch.tensor(X, dtype = torch.float32)\n","<ipython-input-3-cdf7f5c9cc94>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  targets = torch.tensor(targets.to(device), dtype = torch.float32) # mask 되지 않은 값\n","<ipython-input-3-cdf7f5c9cc94>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X = torch.tensor(X, dtype = torch.float32)\n","100%|██████████| 200/200 [03:33<00:00,  1.07s/it]\n","<ipython-input-3-cdf7f5c9cc94>:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X = torch.tensor(X, dtype = torch.float32)\n","<ipython-input-3-cdf7f5c9cc94>:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X = torch.tensor(X, dtype = torch.float32)\n","<ipython-input-3-cdf7f5c9cc94>:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X = torch.tensor(X, dtype = torch.float32)\n"]},{"output_type":"stream","name":"stdout","text":["0.014632497877129261\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"keuC14zoErxd"},"execution_count":null,"outputs":[]}]}